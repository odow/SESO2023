{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Run this cell once"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "%%shell\n",
    "set -e\n",
    "wget -nv https://raw.githubusercontent.com/odow/SESO2023/main/install_colab.sh -O /tmp/install_colab.sh\n",
    "bash /tmp/install_colab.sh  # Takes ~ 2 minutes"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then refresh the page... and run this cell once"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Downloads, Pkg\n",
    "Downloads.download(\"https://raw.githubusercontent.com/odow/SESO2023/main/Project.toml\", \"/tmp/Project.toml\")\n",
    "Pkg.activate(\"/tmp/Project.toml\")\n",
    "Pkg.instantiate()  # Can take ~ 7 minutes"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Two-stage Newsvendor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to model and solve a\n",
    "two-stage stochastic program."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is based on the [Two stage stochastic programs](https://jump.dev/JuMP.jl/dev/tutorials/applications/two_stage_stochastic/)\n",
    "tutorial in JuMP."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial uses the following packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "using SDDP\n",
    "import Distributions\n",
    "import HiGHS\n",
    "import Plots\n",
    "import StatsPlots\n",
    "import Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data for this problem is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "D = Distributions.TriangularDist(150.0, 250.0, 200.0)\n",
    "N = 100\n",
    "d = sort!(rand(D, N));\n",
    "Ω = 1:N\n",
    "P = fill(1 / N, N);\n",
    "StatsPlots.histogram(d; bins = 20, label = \"\", xlabel = \"Demand\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can formulate and train a policy for the two-stage newsvendor problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to construct the graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.LinearGraph(2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we need to write a function which builds a JuMP model for each node in\n",
    "the graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function build_subproblem(subproblem::JuMP.Model, stage::Int)\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if stage == 1\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    else\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can combine the graph and the subproblem builder into a policy graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    build_subproblem,\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d), #= some large upper bound =#\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use `SDDP.train` to construct the policy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check the first-stage buy decision, we need to obtain a decision rule for\n",
    "the first-stage node `1`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "first_stage_rule = SDDP.DecisionRule(model, node = 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can evaluate it, passing in a starting point for the incoming state:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solution = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x => 0.0))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimal value of the state variable is stored here:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solution.outgoing_state[:x]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can simplify the model construction by using `SDDP.LinearPolicyGraph`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    build_subproblem;\n",
    "    stages = 2,\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and we can use Julia's `do` syntax to avoid writing a separate function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = 2,\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem::JuMP.Model, stage::Int\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if stage == 1\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    else\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Querying the decision rules is tedious. It's often more useful to simulate the\n",
    "policy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    10,  #= number of replications =#\n",
    "    [:x, :u_sell, :u_make];  #= variables to record =#\n",
    "    skip_undefined_variables = true,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`simulations` is a vector with 10 elements"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(simulations)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and each element is a vector with two elements (one for each stage)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(simulations[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first stage contains:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second stage contains:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute aggregated statistics across the simulations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "objectives = map(simulations) do simulation\n",
    "    return sum(data[:stage_objective] for data in simulation)\n",
    "end\n",
    "μ, t = SDDP.confidence_interval(objectives)\n",
    "println(\"Simulation ci : $μ ± $t\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risk aversion revisited"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SDDP.jl contains a number of risk measures. One example is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "0.5 * SDDP.Expectation() + 0.5 * SDDP.WorstCase()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can construct a risk-averse policy by passing a risk measure to the\n",
    "`risk_measure` keyword argument of `SDDP.train`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can explore how the optimal decision changes with risk by creating a\n",
    "function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_newsvendor(risk_measure::SDDP.AbstractRiskMeasure)\n",
    "    model = SDDP.LinearPolicyGraph(\n",
    "        stages = 2,\n",
    "        sense = :Max,\n",
    "        upper_bound = 5 * maximum(d),\n",
    "        optimizer = HiGHS.Optimizer,\n",
    "    ) do subproblem, node\n",
    "        @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "        if node == 1\n",
    "            @stageobjective(subproblem, -2 * x.out)\n",
    "        else\n",
    "            @variable(subproblem, u_sell >= 0)\n",
    "            @constraint(subproblem, u_sell <= x.in)\n",
    "            @constraint(subproblem, x.out == x.in - u_sell)\n",
    "            SDDP.parameterize(subproblem, d, P) do ω\n",
    "                set_upper_bound(u_sell, ω)\n",
    "                return\n",
    "            end\n",
    "            @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "        end\n",
    "        return\n",
    "    end\n",
    "    SDDP.train(model; risk_measure = risk_measure, print_level = 0)\n",
    "    first_stage_rule = SDDP.DecisionRule(model; node = 1)\n",
    "    solution = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x => 0.0))\n",
    "    return solution.outgoing_state[:x]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see how many units a decision maker would order using `CVaR`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve_newsvendor(SDDP.CVaR(0.4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "as well as a decision-maker who cares only about the worst-case outcome:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve_newsvendor(SDDP.WorstCase())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, the decision-maker will be somewhere between the two extremes.\n",
    "The `SDDP.Entropic` risk measure is a risk measure that has a single\n",
    "parameter that lets us explore the space of policies between the two extremes.\n",
    "When the parameter is small, the measure acts like `SDDP.Expectation`,\n",
    "and when it is large, it acts like `SDDP.WorstCase`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is what we get if we solve our problem multiple times for different\n",
    "values of the risk aversion parameter $\\gamma$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Γ = [10^i for i in -4:0.5:1]\n",
    "buy = [solve_newsvendor(SDDP.Entropic(γ)) for γ in Γ]\n",
    "Plots.plot(\n",
    "    Γ,\n",
    "    buy;\n",
    "    xaxis = :log,\n",
    "    xlabel = \"Risk aversion parameter γ\",\n",
    "    ylabel = \"Number of pies to make\",\n",
    "    legend = false,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Things to try"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a number of things you can try next:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " * Experiment with different buy and sales prices\n",
    " * Experiment with different distributions of demand\n",
    " * Explore how the optimal policy changes if you use a different risk measure\n",
    " * What happens if you can only buy and sell integer numbers of newspapers?\n",
    "   Try this by adding `Int` to the variable definitions:\n",
    "   `@variable(subproblem, buy >= 0, Int)`\n",
    " * What happens if you use a different upper bound? Try an invalid one like\n",
    "   `-100`, and a very large one like `1e12`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other graphs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two-stage newsvendor problem can be extended to other graphs. For example,\n",
    "here is a three-stage graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.Graph(:root)\n",
    "SDDP.add_node(graph, :make_only)\n",
    "SDDP.add_node(graph, :sell_only)\n",
    "SDDP.add_node(graph, :make_and_sell)\n",
    "SDDP.add_edge(graph, :root => :make_only, 1.0)\n",
    "SDDP.add_edge(graph, :make_only => :make_and_sell, 1.0)\n",
    "SDDP.add_edge(graph, :make_and_sell => :sell_only, 1.0)\n",
    "graph"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "with the model formulation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 2 * 5 * maximum(d),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem::JuMP.Model, node::Symbol\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if node == :make_only\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    elseif node == :sell_only\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    else\n",
    "        @assert node == :make_and_sell\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell + u_make)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out - 2 * u_make)\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "SDDP.train(model)\n",
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    10,  #= number of replications =#\n",
    "    [:x, :u_sell, :u_make];  #= variables to record =#\n",
    "    skip_undefined_variables = true,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try changing the graph. What happens if you add a cycle?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: deterministic to stochastic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_This tutorial was written by Oscar Dowson and Andy Philpott for the 2023\n",
    "Winter School \"Planning under Uncertainty in Energy Markets,\" held March 26 to\n",
    "31 in Geilo, Norway._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Download this tutorial as a notebook.](../../example_reservoir.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to explain how we can go from a deterministic\n",
    "time-staged optimal control model in JuMP to a multistage stochastic\n",
    "optimization model in `SDDP.jl`. As a motivating problem, we consider the\n",
    "hydro-thermal problem with a single reservoir."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For variations on this problem, see the examples and tutorials:\n",
    "\n",
    " * An introduction to SDDP.jl\n",
    " * Hydro-thermal scheduling\n",
    " * Hydro valleys\n",
    " * Infinite horizon hydro-thermal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial requires the following packages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "using SDDP\n",
    "import CSV\n",
    "import DataFrames\n",
    "import HiGHS\n",
    "import Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need some data for the problem. For this tutorial, we'll write CSV\n",
    "files to a temporary directory from Julia. If you have an existing file, you\n",
    "could change the filename to point to that instead."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dir = mktempdir()\n",
    "filename = joinpath(dir, \"example_reservoir.csv\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "csv_data = \"\"\"\n",
    "week,inflow,demand,cost\n",
    "1,3,7,10.2\\n2,2,7.1,10.4\\n3,3,7.2,10.6\\n4,2,7.3,10.9\\n5,3,7.4,11.2\\n\n",
    "6,2,7.6,11.5\\n7,3,7.8,11.9\\n8,2,8.1,12.3\\n9,3,8.3,12.7\\n10,2,8.6,13.1\\n\n",
    "11,3,8.9,13.6\\n12,2,9.2,14\\n13,3,9.5,14.5\\n14,2,9.8,14.9\\n15,3,10.1,15.3\\n\n",
    "16,2,10.4,15.8\\n17,3,10.7,16.2\\n18,2,10.9,16.6\\n19,3,11.2,17\\n20,3,11.4,17.4\\n\n",
    "21,3,11.6,17.7\\n22,2,11.7,18\\n23,3,11.8,18.3\\n24,2,11.9,18.5\\n25,3,12,18.7\\n\n",
    "26,2,12,18.9\\n27,3,12,19\\n28,2,11.9,19.1\\n29,3,11.8,19.2\\n30,2,11.7,19.2\\n\n",
    "31,3,11.6,19.2\\n32,2,11.4,19.2\\n33,3,11.2,19.1\\n34,2,10.9,19\\n35,3,10.7,18.9\\n\n",
    "36,2,10.4,18.8\\n37,3,10.1,18.6\\n38,2,9.8,18.5\\n39,3,9.5,18.4\\n40,3,9.2,18.2\\n\n",
    "41,2,8.9,18.1\\n42,3,8.6,17.9\\n43,2,8.3,17.8\\n44,3,8.1,17.7\\n45,2,7.8,17.6\\n\n",
    "46,3,7.6,17.5\\n47,2,7.4,17.5\\n48,3,7.3,17.5\\n49,2,7.2,17.5\\n50,3,7.1,17.6\\n\n",
    "51,3,7,17.7\\n52,3,7,17.8\\n\n",
    "\"\"\"\n",
    "write(filename, csv_data);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here we read it into a DataFrame:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data = CSV.read(filename, DataFrames.DataFrame)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's easier to visualize the data if we plot it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Plots.plot(\n",
    "    Plots.plot(data[!, :inflow], ylabel = \"Inflow\"),\n",
    "    Plots.plot(data[!, :demand], ylabel = \"Demand\"),\n",
    "    Plots.plot(data[!, :cost], ylabel = \"Cost\", xlabel = \"Week\");\n",
    "    layout = (3, 1),\n",
    "    legend = false,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of weeks will be useful later:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "T = size(data, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deterministic JuMP model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start, we construct a deterministic model in pure JuMP."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a JuMP model, using `HiGHS` as the optimizer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Model(HiGHS.Optimizer)\n",
    "set_silent(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`x_storage[t]`: the amount of water in the reservoir at the start of stage `t`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reservoir_max = 320.0\n",
    "@variable(model, 0 <= x_storage[1:T+1] <= reservoir_max)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need an initial condition for `x_storage[1]`. Fix it to 300 units:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reservoir_initial = 300\n",
    "fix(x_storage[1], reservoir_initial; force = true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`u_flow[t]`: the amount of water to flow through the turbine in stage `t`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "flow_max = 12\n",
    "@variable(model, 0 <= u_flow[1:T] <= flow_max)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`u_spill[t]`: the amount of water to spill from the reservoir in stage `t`,\n",
    "bypassing the turbine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variable(model, 0 <= u_spill[1:T])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`u_thermal[t]`: the amount of thermal generation in stage `t`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variable(model, 0 <= u_thermal[1:T])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`ω_inflow[t]`: the amount of inflow to the reservoir in stage `t`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variable(model, ω_inflow[1:T])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this model, our inflow is fixed, so we fix it to the data we have:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for t in 1:T\n",
    "    fix(ω_inflow[t], data[t, :inflow])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The water balance constraint says that the water in the reservoir at the start\n",
    "of stage `t+1` is the water in the reservoir at the start of stage `t`, less\n",
    "the amount flowed through the turbine, `u_flow[t]`, less the amount spilled,\n",
    "`u_spill[t]`, plus the amount of inflow, `ω_inflow[t]`, into the reservoir:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@constraint(\n",
    "    model,\n",
    "    [t in 1:T],\n",
    "    x_storage[t+1] == x_storage[t] - u_flow[t] - u_spill[t] + ω_inflow[t],\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need a `supply = demand` constraint. In practice, the units of this\n",
    "would be in MWh, and there would be a conversion factor between the amount of\n",
    "water flowing through the turbine and the power output. To simplify, we assume\n",
    "that power and water have the same units, so that one \"unit\" of demand is\n",
    "equal to one \"unit\" of the reservoir `x_storage[t]`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@constraint(model, [t in 1:T], u_flow[t] + u_thermal[t] == data[t, :demand])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our objective is to minimize the cost of thermal generation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@objective(model, Min, sum(data[t, :cost] * u_thermal[t] for t in 1:T))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's optimize and check the solution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "optimize!(model)\n",
    "solution_summary(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The total cost is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "objective_value(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a plot of demand and generation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Plots.plot(data[!, :demand]; label = \"Demand\", xlabel = \"Week\")\n",
    "Plots.plot!(value.(u_thermal), label = \"Thermal\")\n",
    "Plots.plot!(value.(u_flow), label = \"Hydro\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here's the storage over time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Plots.plot(value.(x_storage); label = \"Storage\", xlabel = \"Week\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deterministic SDDP model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the next step, we show how to decompose our JuMP model into SDDP.jl. It\n",
    "should obtain the same solution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    stages = T,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    @variable(\n",
    "        sp,\n",
    "        0 <= x_storage <= reservoir_max,\n",
    "        SDDP.State,\n",
    "        initial_value = reservoir_initial,\n",
    "    )\n",
    "    @variable(sp, 0 <= u_flow <= flow_max)\n",
    "    @variable(sp, 0 <= u_thermal)\n",
    "    @variable(sp, 0 <= u_spill)\n",
    "    @variable(sp, ω_inflow)\n",
    "    fix(ω_inflow, data[t, :inflow])\n",
    "    @constraint(sp, x_storage.out == x_storage.in - u_flow - u_spill + ω_inflow)\n",
    "    @constraint(sp, u_flow + u_thermal == data[t, :demand])\n",
    "    @stageobjective(sp, data[t, :cost] * u_thermal)\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can you see how the JuMP model maps to this syntax? We have created a\n",
    "`SDDP.LinearPolicyGraph` with `T` stages, we're minimizing, and we're\n",
    "using `HiGHS.Optimizer` as the optimizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A few bits might be non-obvious:\n",
    "\n",
    "* We need to provide a lower bound for the objective function. Since our costs\n",
    "  are always positive, a valid lower bound for the total cost is `0.0`.\n",
    "* We define `x_storage` as a state variable using `SDDP.State`. A state\n",
    "  variable is any variable that flows through time, and for which we need to\n",
    "  know the value of it in stage `t-1` to compute the best action in stage `t`.\n",
    "  The state variable `x_storage` is actually two decision variables,\n",
    "  `x_storage.in` and `x_storage.out`, which represent `x_storage[t]` and\n",
    "  `x_storage[t+1]` respectively.\n",
    "* We need to use `@stageobjective` instead of `@objective`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of calling `JuMP.optimize!`, SDDP.jl uses a `train` method. With our\n",
    "machine learning hat on, you can think of SDDP.jl as training a function for\n",
    "each stage that accepts the current reservoir state as input and returns the\n",
    "optimal actions as output. It is also an iterative algorithm, so we need to\n",
    "specify when it should terminate:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model; iteration_limit = 10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a quick sanity check, did we get the same cost as our JuMP model?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.calculate_bound(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's good. Next, to check the value of the decision variables. This isn't as\n",
    "straight forward as our JuMP model. Instead, we need to _simulate_ the policy,\n",
    "and then extract the values of the decision variables from the results of the\n",
    "simulation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our model is deterministic, we need only 1 replication of the\n",
    "simulation, and we want to record the values of the `x_storage`, `u_flow`, and\n",
    "`u_thermal` variables:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    1,  # Number of replications\n",
    "    [:x_storage, :u_flow, :u_thermal],\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `simulations` vector is too big to show. But it contains one element for\n",
    "each replication, and each replication contains one dictionary for each stage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, the data corresponding to the tenth stage in the first\n",
    "replication is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][10]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's grab the trace of the `u_thermal` and `u_flow` variables in the first\n",
    "replication, and then plot them:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_sim = [sim[:u_thermal] for sim in simulations[1]]\n",
    "u_sim = [sim[:u_flow] for sim in simulations[1]]\n",
    "\n",
    "Plots.plot(data[!, :demand]; label = \"Demand\", xlabel = \"Week\")\n",
    "Plots.plot!(r_sim, label = \"Thermal\")\n",
    "Plots.plot!(u_sim, label = \"Hydro\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perfect. That's the same as we got before."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's look at `x_storage`. This is a little more complicated, because we\n",
    "need to grab the outgoing value of the state variable in each stage:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_sim = [sim[:x_storage].out for sim in simulations[1]]\n",
    "\n",
    "Plots.plot(x_sim; label = \"Storage\", xlabel = \"Week\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic SDDP model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we add some randomness to our model. In each stage, we assume that the\n",
    "inflow could be: 2 units lower, with 30% probability; the same as before, with\n",
    "40% probability; or 5 units higher, with 30% probability."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    stages = T,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    @variable(\n",
    "        sp,\n",
    "        0 <= x_storage <= reservoir_max,\n",
    "        SDDP.State,\n",
    "        initial_value = reservoir_initial,\n",
    "    )\n",
    "    @variable(sp, 0 <= u_flow <= flow_max)\n",
    "    @variable(sp, 0 <= u_thermal)\n",
    "    @variable(sp, 0 <= u_spill)\n",
    "    @variable(sp, ω_inflow)\n",
    "    # <--- This bit is new\n",
    "    Ω, P = [-2, 0, 5], [0.3, 0.4, 0.3]\n",
    "    SDDP.parameterize(sp, Ω, P) do ω\n",
    "        fix(ω_inflow, data[t, :inflow] + ω)\n",
    "        return\n",
    "    end\n",
    "    # --->\n",
    "    @constraint(sp, x_storage.out == x_storage.in - u_flow - u_spill + ω_inflow)\n",
    "    @constraint(sp, u_flow + u_thermal == data[t, :demand])\n",
    "    @stageobjective(sp, data[t, :cost] * u_thermal)\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can you see the differences?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train our new model. We need more iterations because of the\n",
    "stochasticity:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model; iteration_limit = 100)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now simulate the policy. This time we do 100 replications because the policy\n",
    "is now stochastic instead of deterministic:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations =\n",
    "    SDDP.simulate(model, 100, [:x_storage, :u_flow, :u_thermal, :ω_inflow]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's plot the use of thermal generation in each replication:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot = Plots.plot(data[!, :demand]; label = \"Demand\", xlabel = \"Week\")\n",
    "for simulation in simulations\n",
    "    Plots.plot!(plot, [sim[:u_thermal] for sim in simulation], label = \"\")\n",
    "end\n",
    "plot"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Viewing an interpreting static plots like this is difficult, particularly as\n",
    "the number of simulations grows. SDDP.jl includes an interactive\n",
    "`SpaghettiPlot` that makes things easier:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot = SDDP.SpaghettiPlot(simulations)\n",
    "SDDP.add_spaghetti(plot; title = \"Storage\") do sim\n",
    "    return sim[:x_storage].out\n",
    "end\n",
    "SDDP.add_spaghetti(plot; title = \"Hydro\") do sim\n",
    "    return sim[:u_flow]\n",
    "end\n",
    "SDDP.add_spaghetti(plot; title = \"Inflow\") do sim\n",
    "    return sim[:ω_inflow]\n",
    "end\n",
    "SDDP.plot(\n",
    "    plot,\n",
    "    \"spaghetti_plot.html\";\n",
    "    # We need this to build the documentation. Set to true if running locally.\n",
    "    open = false,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "```@raw html\n",
    "<iframe src=\"../spaghetti_plot.html\" style=\"width:100%;height:500px;\"></iframe>\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Info**\n",
    "    If you have trouble viewing the plot, you can\n",
    "    [open it in a new window](../../spaghetti_plot.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take this model and modify it following the suggestions below. The SDDP.jl\n",
    "documentation has a range of similar examples and hints for how to achieve\n",
    "them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Terminal value functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model ends with an empty reservoir. That isn't ideal for the following\n",
    "year. Can you modify the objective in the final stage to encourage ending the\n",
    "year with a full reservoir?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might write some variation of:\n",
    "\n",
    "```julia\n",
    "if t == 52\n",
    "    @variable(sp, terminal_cost_function >= 0)\n",
    "    @constraint(sp, terminal_cost_function >= reservoir_initial - x_storage.out)\n",
    "    @stageobjective(sp, data[t, :cost] * u_thermal + terminal_cost_function)\n",
    "else\n",
    "    @stageobjective(sp, data[t, :cost] * u_thermal)\n",
    "end\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Higher fidelity modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model is very basic. There are many aspects that we could improve:\n",
    "\n",
    "* Instead of hard-coding a terminal value function, can you solve an infinite\n",
    "  horizon model? What are the differences?\n",
    "\n",
    "* Can you add a second reservoir to make a river chain?\n",
    "\n",
    "* Can you modify the problem and data to use proper units, including a\n",
    "  conversion between the volume of water flowing through the turbine and the\n",
    "  electrical power output?\n",
    "\n",
    "* Can you add random demand or cost data as well as inflows?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithmic considerations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The algorithm implemented by SDDP.jl has a number of tuneable parameters:\n",
    "\n",
    "* Try using a different lower bound. What happens if it is too low, or too\n",
    "  high?\n",
    "\n",
    "* Was our stopping rule correct? What happens if we use fewer or more\n",
    "  iterations? What other stopping rules could you try?\n",
    "\n",
    "* Can you add a risk measure to make the policy risk-averse?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: the milk producer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to fit a Markovian policy\n",
    "graph to a univariate stochastic process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial uses the following packages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SDDP\n",
    "import HiGHS\n",
    "import Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A company produces milk for sale on a spot market each month. The quantity of\n",
    "milk they produce is uncertain, and so too is the price on the spot market.\n",
    "The company can store unsold milk in a stockpile of dried milk powder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The spot price is determined by an auction system, and so varies from month to\n",
    "month, but demonstrates serial correlation. In each auction, there is\n",
    "sufficient demand that the milk producer finds a buyer for all their\n",
    "milk, regardless of the quantity they supply. Furthermore, the spot price\n",
    "is independent of the milk producer (they are a small player in the market)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The spot price is highly volatile, and is the result of a process that is out\n",
    "of the control of the company. To counteract their price risk, the company\n",
    "engages in a forward contracting programme."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The forward contracting programme is a deal for physical milk four months in\n",
    "the future."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The futures price is the current spot price, plus some forward contango (the\n",
    "buyers gain certainty that they will receive the milk in the future)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, the milk company should forward contract (since they reduce\n",
    "their price risk), however they also have production risk. Therefore, it may\n",
    "be the case that they forward contract a fixed amount, but find that they do\n",
    "not produce enough milk to meet the fixed demand. They are then forced to\n",
    "buy additional milk on the spot market."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of the milk company is to choose the extent to which they forward\n",
    "contract in order to maximise (risk-adjusted) revenues, whilst managing their\n",
    "production risk."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A stochastic process for price"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is outside the scope of this tutorial, but assume that we have gone away\n",
    "and analysed historical data to fit a stochastic process to the sequence of\n",
    "monthly auction spot prices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One plausible model is a multiplicative auto-regressive model of order one,\n",
    "where the white noise term is modeled by a finite distribution of empirical\n",
    "residuals. We can simulate this stochastic process as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function simulator()\n",
    "    residuals = [0.0987, 0.199, 0.303, 0.412, 0.530, 0.661, 0.814, 1.010, 1.290]\n",
    "    residuals = 0.1 * vcat(-residuals, 0.0, residuals)\n",
    "    scenario = zeros(12)\n",
    "    y, μ, α = 4.5, 6.0, 0.05\n",
    "    for t in 1:12\n",
    "        y = exp((1 - α) * log(y) + α * log(μ) + rand(residuals))\n",
    "        scenario[t] = clamp(y, 3.0, 9.0)\n",
    "    end\n",
    "    return scenario\n",
    "end\n",
    "\n",
    "simulator()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It may be helpful to visualize a number of simulations of the price process:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot = Plots.plot(\n",
    "    [simulator() for _ in 1:500];\n",
    "    color = \"gray\",\n",
    "    opacity = 0.2,\n",
    "    legend = false,\n",
    "    xlabel = \"Month\",\n",
    "    ylabel = \"Price [\\$/kg]\",\n",
    "    xlims = (1, 12),\n",
    "    ylims = (3, 9),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The prices gradually revert to the mean of \\$6/kg, and there is high\n",
    "volatility."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can't incorporate this price process directly into SDDP.jl, but we can fit\n",
    "a `SDDP.MarkovianGraph` directly from the simulator:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.MarkovianGraph(simulator; budget = 30, scenarios = 10_000);\n",
    "nothing  # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here `budget` is the number of nodes in the policy graph, and `scenarios` is\n",
    "the number of simulations to use when estimating the transition probabilities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph contains too many nodes to be show, but we can plot it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for ((t, price), edges) in graph.nodes\n",
    "    for ((t′, price′), probability) in edges\n",
    "        Plots.plot!(\n",
    "            plot,\n",
    "            [t, t′],\n",
    "            [price, price′];\n",
    "            color = \"red\",\n",
    "            width = 3 * probability,\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "plot"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks okay. Try changing `budget` and `scenarios` to see how different\n",
    "Markovian policy graphs can be created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a Markovian graph, we can build the model. See if you can\n",
    "work out how we arrived at this formulation by reading the background\n",
    "description. Do all the variables and constraints make sense?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 1e2,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, node\n",
    "    # Decompose the node into the month (::Int) and spot price (::Float64)\n",
    "    t, price = node::Tuple{Int,Float64}\n",
    "    # Transactions on the futures market cost 0.01\n",
    "    c_transaction = 0.01\n",
    "    # It costs the company +50% to buy milk on the spot market and deliver to\n",
    "    # their customers\n",
    "    c_buy_premium = 1.5\n",
    "    # Buyer is willing to pay +5% for certainty\n",
    "    c_contango = 1.05\n",
    "    # Distribution of production\n",
    "    Ω_production = range(0.1, 0.2; length = 5)\n",
    "    c_max_production = 12 * maximum(Ω_production)\n",
    "    # x_stock: quantity of milk in stock pile\n",
    "    @variable(sp, 0 <= x_stock, SDDP.State, initial_value = 0)\n",
    "    # x_forward[i]: quantity of milk for delivery in i months\n",
    "    @variable(sp, 0 <= x_forward[1:4], SDDP.State, initial_value = 0)\n",
    "    # u_spot_sell: quantity of milk to sell on spot market\n",
    "    @variable(sp, 0 <= u_spot_sell <= c_max_production)\n",
    "    # u_spot_buy: quantity of milk to buy on spot market\n",
    "    @variable(sp, 0 <= u_spot_buy <= c_max_production)\n",
    "    # u_spot_buy: quantity of milk to sell on futures market\n",
    "    c_max_futures = t <= 8 ? c_max_production : 0.0\n",
    "    @variable(sp, 0 <= u_forward_sell <= c_max_futures)\n",
    "    # ω_production: production random variable\n",
    "    @variable(sp, ω_production)\n",
    "    # Forward contracting constraints:\n",
    "    @constraint(sp, [i in 1:3], x_forward[i].out == x_forward[i+1].in)\n",
    "    @constraint(sp, x_forward[4].out == u_forward_sell)\n",
    "    # Stockpile balance constraint\n",
    "    @constraint(\n",
    "        sp,\n",
    "        x_stock.out ==\n",
    "        x_stock.in + ω_production + u_spot_buy - x_forward[1].in - u_spot_sell\n",
    "    )\n",
    "    # The random variables. `price` comes from the Markov node\n",
    "    Ω = [(price, p) for p in Ω_production]\n",
    "    SDDP.parameterize(sp, Ω) do ω::Tuple{Float64,Float64}\n",
    "        # Fix the ω_production variable\n",
    "        fix(ω_production, ω[2])\n",
    "        @stageobjective(\n",
    "            sp,\n",
    "            # Sales on spot market\n",
    "            ω[1] * (u_spot_sell - c_buy_premium * u_spot_buy) +\n",
    "            # Sales on futures smarket\n",
    "            (ω[1] * c_contango - c_transaction) * u_forward_sell\n",
    "        )\n",
    "        return\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a model, we train a policy. The `SDDP.SimulatorSamplingScheme`\n",
    "is used in the forward pass. It generates an out-of-sample sequence of prices\n",
    "using `simulator` and traverses the closest sequence of nodes in the policy\n",
    "graph. When calling `SDDP.parameterize` for each subproblem, it uses\n",
    "the new out-of-sample price instead of the price associated with the Markov\n",
    "node."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(\n",
    "    model;\n",
    "    time_limit = 20,\n",
    "    risk_measure = 0.5 * SDDP.Expectation() + 0.5 * SDDP.AVaR(0.25),\n",
    "    sampling_scheme = SDDP.SimulatorSamplingScheme(simulator),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Warning**\n",
    "    We're intentionally terminating the training early so that the\n",
    "    documentation doesn't take too long to build. If you run this example\n",
    "    locally, increase the time limit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When simulating the policy, we can also use the\n",
    "`SDDP.SimulatorSamplingScheme`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    200,\n",
    "    Symbol[:x_stock, :u_forward_sell, :u_spot_sell, :u_spot_buy];\n",
    "    sampling_scheme = SDDP.SimulatorSamplingScheme(simulator),\n",
    ");\n",
    "nothing  # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To show how the sampling scheme uses the new out-of-sample price instead of\n",
    "the price associated with the Markov node, compare the index of the Markov\n",
    "state visited in stage 12 of the first simulation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][12][:node_index]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "to the realization of the noise `(price, ω)` passed to `SDDP.parameterize`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][12][:noise_term]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can plot the policy to gain insight (although note that we\n",
    "terminated the training early, so we should run the re-train the policy for\n",
    "more iterations before making too many judgements)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot = Plots.plot(\n",
    "    SDDP.publication_plot(simulations; title = \"x_stock.out\") do data\n",
    "        return data[:x_stock].out\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations; title = \"u_forward_sell\") do data\n",
    "        return data[:u_forward_sell]\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations; title = \"u_spot_buy\") do data\n",
    "        return data[:u_spot_buy]\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations; title = \"u_spot_sell\") do data\n",
    "        return data[:u_spot_sell]\n",
    "    end;\n",
    "    layout = (2, 2),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Train the policy for longer. What do you observe?\n",
    "* Try creating different Markovian graphs. What happens if you add more nodes?\n",
    "* Try different risk measures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introductory theory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "    This tutorial is aimed at advanced undergraduates or early-stage graduate\n",
    "    students. You don't need prior exposure to stochastic programming!\n",
    "    (Indeed, it may be better if you don't, because our approach is\n",
    "    non-standard in the literature.)\n",
    "\n",
    "    This tutorial is also a living document. If parts are unclear, please\n",
    "    [open an issue](https://github.com/odow/SDDP.jl/issues/new) so it can be\n",
    "    improved!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial will teach you how the stochastic dual dynamic programming\n",
    "algorithm works by implementing a simplified version of the algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our implementation is very much a \"vanilla\" version of SDDP; it doesn't have\n",
    "(m)any fancy computational tricks (e.g., the ones included in SDDP.jl) that\n",
    "you need to code a performant or stable version that will work on realistic\n",
    "instances. However, our simplified implementation will work on arbitrary\n",
    "policy graphs, including those with cycles such as infinite horizon problems!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Packages**\n",
    "\n",
    "This tutorial uses the following packages. For clarity, we call\n",
    "`import PackageName` so that we must prefix `PackageName.` to all functions\n",
    "and structs provided by that package. Everything not prefixed is either part\n",
    "of base Julia, or we wrote it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import ForwardDiff\n",
    "import HiGHS\n",
    "import JuMP\n",
    "import Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tip**\n",
    "    You can follow along by installing the above packages, and copy-pasting\n",
    "    the code we will write into a Julia REPL. Alternatively, you can download\n",
    "    the Julia `.jl` file which created this tutorial [from GitHub](https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/21_theory_intro.jl)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries: background theory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start this tutorial by reading An introduction to SDDP.jl, which\n",
    "introduces the necessary notation and vocabulary that we need for this\n",
    "tutorial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries: Kelley's cutting plane algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kelley's cutting plane algorithm is an iterative method for minimizing convex\n",
    "functions. Given a convex function $f(x)$, Kelley's constructs an\n",
    "under-approximation of the function at the minimum by a set of first-order\n",
    "Taylor series approximations (called **cuts**) constructed at a set of points\n",
    "$k = 1,\\ldots,K$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^K = \\min\\limits_{\\theta \\in \\mathbb{R}, x \\in \\mathbb{R}^N} \\;\\; & \\theta\\\\\n",
    "& \\theta \\ge f(x_k) + \\frac{d}{dx}f(x_k)^\\top (x - x_k),\\quad k=1,\\ldots,K\\\\\n",
    "& \\theta \\ge M,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $M$ is a sufficiently large negative number that is a lower bound for\n",
    "$f$ over the domain of $x$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kelley's cutting plane algorithm is a structured way of choosing points $x_k$\n",
    "to visit, so that as more cuts are added:\n",
    "$$\n",
    "\\lim_{K \\rightarrow \\infty} f^K = \\min\\limits_{x \\in \\mathbb{R}^N} f(x)\n",
    "$$\n",
    "However, before we introduce the algorithm, we need to introduce some bounds."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bounds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By convexity, $f^K \\le f(x)$ for all $x$. Thus, if $x^*$ is a minimizer of\n",
    "$f$, then at any point in time we can construct a lower bound for $f(x^*)$ by\n",
    "solving $f^K$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, we can use the primal solutions $x_k^*$ returned by solving $f^k$ to\n",
    "evaluate $f(x_k^*)$ to generate an upper bound."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore, $f^K \\le f(x^*) \\le \\min\\limits_{k=1,\\ldots,K} f(x_k^*)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the lower bound is sufficiently close to the upper bound, we can\n",
    "terminate the algorithm and declare that we have found an solution that is\n",
    "close to optimal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is pseudo-code fo the Kelley algorithm:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Take as input a convex function $f(x)$ and a iteration limit $K_{max}$.\n",
    "   Set $K = 0$, and initialize $f^K$. Set $lb = -\\infty$ and $ub = \\infty$.\n",
    "2. Solve $f^K$ to obtain a candidate solution $x_{K+1}$.\n",
    "3. Update $lb = f^K$ and $ub = \\min\\{ub, f(x_{K+1})\\}$.\n",
    "4. Add a cut $\\theta \\ge f(x_{K+1}) + \\frac{d}{dx}f\\left(x_{K+1}\\right)^\\top (x - x_{K+1})$ to form $f^{K+1}$.\n",
    "5. Increment $K$.\n",
    "6. If $K = K_{max}$ or $|ub - lb| < \\epsilon$, STOP, otherwise, go to step 2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here's a complete implementation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function kelleys_cutting_plane(\n",
    "    # The function to be minimized.\n",
    "    f::Function,\n",
    "    # The gradient of `f`. By default, we use automatic differentiation to\n",
    "    # compute the gradient of f so the user doesn't have to!\n",
    "    dfdx::Function = x -> ForwardDiff.gradient(f, x);\n",
    "    # The number of arguments to `f`.\n",
    "    input_dimension::Int,\n",
    "    # A lower bound for the function `f` over its domain.\n",
    "    lower_bound::Float64,\n",
    "    # The number of iterations to run Kelley's algorithm for before stopping.\n",
    "    iteration_limit::Int,\n",
    "    # The absolute tolerance ϵ to use for convergence.\n",
    "    tolerance::Float64 = 1e-6,\n",
    ")\n",
    "    # Step (1):\n",
    "    K = 0\n",
    "    model = JuMP.Model(HiGHS.Optimizer)\n",
    "    JuMP.set_silent(model)\n",
    "    JuMP.@variable(model, θ >= lower_bound)\n",
    "    JuMP.@variable(model, x[1:input_dimension])\n",
    "    JuMP.@objective(model, Min, θ)\n",
    "    x_k = fill(NaN, input_dimension)\n",
    "    lower_bound, upper_bound = -Inf, Inf\n",
    "    while true\n",
    "        # Step (2):\n",
    "        JuMP.optimize!(model)\n",
    "        x_k .= JuMP.value.(x)\n",
    "        # Step (3):\n",
    "        lower_bound = JuMP.objective_value(model)\n",
    "        upper_bound = min(upper_bound, f(x_k))\n",
    "        println(\"K = $K : $(lower_bound) <= f(x*) <= $(upper_bound)\")\n",
    "        # Step (4):\n",
    "        JuMP.@constraint(model, θ >= f(x_k) + dfdx(x_k)' * (x .- x_k))\n",
    "        # Step (5):\n",
    "        K = K + 1\n",
    "        # Step (6):\n",
    "        if K == iteration_limit\n",
    "            println(\"-- Termination status: iteration limit --\")\n",
    "            break\n",
    "        elseif abs(upper_bound - lower_bound) < tolerance\n",
    "            println(\"-- Termination status: converged --\")\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    println(\"Found solution: x_K = \", x_k)\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's run our algorithm to see what happens:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "kelleys_cutting_plane(\n",
    "    input_dimension = 2,\n",
    "    lower_bound = 0.0,\n",
    "    iteration_limit = 20,\n",
    ") do x\n",
    "    return (x[1] - 1)^2 + (x[2] + 2)^2 + 1.0\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Warning**\n",
    "    It's hard to choose a valid lower bound! If you choose one too loose, the\n",
    "    algorithm can take a long time to converge. However, if you choose one so\n",
    "    tight that $M > f(x^*)$, then you can obtain a suboptimal solution. For a\n",
    "    deeper discussion of the implications for SDDP.jl, see\n",
    "    Choosing an initial bound."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries: approximating the cost-to-go term"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the background theory section, we discussed how you could formulate an\n",
    "optimal policy to a multistage stochastic program using the dynamic\n",
    "programming recursion:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_i(x, \\omega) = \\min\\limits_{\\bar{x}, x^\\prime, u} \\;\\; & C_i(\\bar{x}, u, \\omega) + \\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}[V_j(x^\\prime, \\varphi)]\\\\\n",
    "& x^\\prime = T_i(\\bar{x}, u, \\omega) \\\\\n",
    "& u \\in U_i(\\bar{x}, \\omega) \\\\\n",
    "& \\bar{x} = x,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where our decision rule, $\\pi_i(x, \\omega)$, solves this optimization problem\n",
    "and returns a $u^*$ corresponding to an optimal solution. Moreover, we alluded\n",
    "to the fact that the cost-to-go term (the nasty recursive expectation) makes\n",
    "this problem intractable to solve."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, if, excluding the cost-to-go term (i.e., the `SP` formulation),\n",
    "$V_i(x, \\omega)$ can be formulated as a linear program (this also works for\n",
    "convex programs, but the math is more involved), then we can make some\n",
    "progress by noticing that $x$ only appears as a right-hand side term of the\n",
    "fishing constraint $\\bar{x} = x$. Therefore, $V_i(x, \\cdot)$ is convex with\n",
    "respect to $x$ for fixed $\\omega$. (If you have not seen this result before,\n",
    "try to prove it.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fishing constraint $\\bar{x} = x$ has an associated dual variable. The\n",
    "economic interpretation of this dual variable is that it represents the change\n",
    "in the objective function if the right-hand side $x$ is increased on the scale\n",
    "of one unit. In other words, and with a slight abuse of notation, it is the\n",
    "value $\\frac{d}{dx} V_i(x, \\omega)$. (Because $V_i$ is not differentiable, it\n",
    "is a [subgradient](https://en.wikipedia.org/wiki/Subderivative) instead of a\n",
    "derivative.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we implement the constraint $\\bar{x} = x$ by setting the lower- and upper\n",
    "bounds of $\\bar{x}$ to $x$, then the [reduced cost](https://en.wikipedia.org/wiki/Reduced_cost)\n",
    "of the decision variable $\\bar{x}$ is the subgradient, and we do not need to\n",
    "explicitly add the fishing constraint as a row to the constraint matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tip**\n",
    "    The subproblem can have binary and integer variables, but you'll need to\n",
    "    use Lagrangian duality to compute a subgradient!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastic dual dynamic programming converts this problem into a tractable\n",
    "form by applying Kelley's cutting plane algorithm to the $V_j$ functions in\n",
    "the cost-to-go term:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_i^K(x, \\omega) = \\min\\limits_{\\bar{x}, x^\\prime, u} \\;\\; & C_i(\\bar{x}, u, \\omega) + \\theta\\\\\n",
    "& x^\\prime = T_i(\\bar{x}, u, \\omega) \\\\\n",
    "& u \\in U_i(\\bar{x}, \\omega) \\\\\n",
    "& \\bar{x} = x \\\\\n",
    "& \\theta \\ge \\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}\\left[V_j^k(x^\\prime_k, \\varphi) + \\frac{d}{dx^\\prime}V_j^k(x^\\prime_k, \\varphi)^\\top (x^\\prime - x^\\prime_k)\\right],\\quad k=1,\\ldots,K \\\\\n",
    "& \\theta \\ge M.\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All we need now is a way of generating these cutting planes in an iterative\n",
    "manner. Before we get to that though, let's start writing some code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make a start by defining the problem structure. Like SDDP.jl, we need a\n",
    "few things:\n",
    "\n",
    "1. A description of the structure of the policy graph: how many nodes there\n",
    "   are, and the arcs linking the nodes together with their corresponding\n",
    "   probabilities.\n",
    "2. A JuMP model for each node in the policy graph.\n",
    "3. A way to identify the incoming and outgoing state variables of each node.\n",
    "4. A description of the random variable, as well as a function that we can\n",
    "   call that will modify the JuMP model to reflect the realization of the\n",
    "   random variable.\n",
    "5. A decision variable to act as the approximated cost-to-go term."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Warning**\n",
    "    In the interests of brevity, there is minimal error checking. Think about\n",
    "    all the different ways you could break the code!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Structs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first struct we are going to use is a `State` struct that will wrap an\n",
    "incoming and outgoing state variable:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct State\n",
    "    in::JuMP.VariableRef\n",
    "    out::JuMP.VariableRef\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need a struct to wrap all of the uncertainty within a node:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Uncertainty\n",
    "    parameterize::Function\n",
    "    Ω::Vector{Any}\n",
    "    P::Vector{Float64}\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`parameterize` is a function which takes a realization of the random variable\n",
    "$\\omega\\in\\Omega$ and updates the subproblem accordingly. The finite discrete\n",
    "random variable is defined by the vectors `Ω` and `P`, so that the random\n",
    "variable takes the value `Ω[i]` with probability `P[i]`. As such, `P` should\n",
    "sum to 1. (We don't check this here, but we should; we do in SDDP.jl.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have two building blocks, we can declare the structure of each node:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Node\n",
    "    subproblem::JuMP.Model\n",
    "    states::Dict{Symbol,State}\n",
    "    uncertainty::Uncertainty\n",
    "    cost_to_go::JuMP.VariableRef\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* `subproblem` is going to be the JuMP model that we build at each node.\n",
    "* `states` is a dictionary that maps a symbolic name of a state variable to a\n",
    "  `State` object wrapping the incoming and outgoing state variables in\n",
    "  `subproblem`.\n",
    "* `uncertainty` is an `Uncertainty` object described above.\n",
    "* `cost_to_go` is a JuMP variable that approximates the cost-to-go term."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we define a simplified policy graph as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct PolicyGraph\n",
    "    nodes::Vector{Node}\n",
    "    arcs::Vector{Dict{Int,Float64}}\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a vector of nodes, as well as a data structure for the arcs. `arcs`\n",
    "is a vector of dictionaries, where `arcs[i][j]` gives the probability of\n",
    "transitioning from node `i` to node `j`, if an arc exists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To simplify things, we will assume that the root node transitions to node `1`\n",
    "with probability 1, and there are no other incoming arcs to node 1. Notably,\n",
    "we can still define cyclic graphs though!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also define a nice `show` method so that we don't accidentally print a\n",
    "large amount of information to the screen when creating a model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function Base.show(io::IO, model::PolicyGraph)\n",
    "    println(io, \"A policy graph with $(length(model.nodes)) nodes\")\n",
    "    println(io, \"Arcs:\")\n",
    "    for (from, arcs) in enumerate(model.arcs)\n",
    "        for (to, probability) in arcs\n",
    "            println(io, \"  $(from) => $(to) w.p. $(probability)\")\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have some basic types, let's implement some functions so that the user\n",
    "can create a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need an example of a function that the user will provide. Like\n",
    "SDDP.jl, this takes an empty `subproblem`, and a node index, in this case\n",
    "`t::Int`. You could change this function to change the model, or define a new\n",
    "one later in the code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going to copy the example from An introduction to SDDP.jl,\n",
    "with some minor adjustments for the fact we don't have many of the bells and\n",
    "whistles of SDDP.jl. You can probably see how some of the SDDP.jl\n",
    "functionality like `@stageobjective` and `SDDP.parameterize`\n",
    "help smooth some of the usability issues like needing to construct both the\n",
    "incoming and outgoing state variables, or needing to explicitly declare\n",
    "`return states, uncertainty`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::JuMP.Model, t::Int)\n",
    "    # Define the state variables. Note how we fix the incoming state to the\n",
    "    # initial state variable regardless of `t`! This isn't strictly necessary;\n",
    "    # it only matters that we do it for the first node.\n",
    "    JuMP.@variable(subproblem, volume_in == 200)\n",
    "    JuMP.@variable(subproblem, 0 <= volume_out <= 200)\n",
    "    states = Dict(:volume => State(volume_in, volume_out))\n",
    "    # Define the control variables.\n",
    "    JuMP.@variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "        inflow\n",
    "    end)\n",
    "    # Define the constraints\n",
    "    JuMP.@constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume_out == volume_in + inflow - hydro_generation - hydro_spill\n",
    "            demand_constraint, thermal_generation + hydro_generation == 150.0\n",
    "        end\n",
    "    )\n",
    "    # Define the objective for each stage `t`. Note that we can use `t` as an\n",
    "    # index for t = 1, 2, 3.\n",
    "    fuel_cost = [50.0, 100.0, 150.0]\n",
    "    JuMP.@objective(subproblem, Min, fuel_cost[t] * thermal_generation)\n",
    "    # Finally, we define the uncertainty object. Because this is a simplified\n",
    "    # implementation of SDDP, we shall politely ask the user to only modify the\n",
    "    # constraints, and not the objective function! (Not that it changes the\n",
    "    # algorithm, we just have to add more information to keep track of things.)\n",
    "    uncertainty = Uncertainty([0.0, 50.0, 100.0], [1 / 3, 1 / 3, 1 / 3]) do ω\n",
    "        return JuMP.fix(inflow, ω)\n",
    "    end\n",
    "    return states, uncertainty\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next function we need to define is the analog of\n",
    "`SDDP.PolicyGraph`. It should be pretty readable."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function PolicyGraph(\n",
    "    subproblem_builder::Function;\n",
    "    graph::Vector{Dict{Int,Float64}},\n",
    "    lower_bound::Float64,\n",
    "    optimizer,\n",
    ")\n",
    "    nodes = Node[]\n",
    "    for t in 1:length(graph)\n",
    "        # Create a model.\n",
    "        model = JuMP.Model(optimizer)\n",
    "        JuMP.set_silent(model)\n",
    "        # Use the provided function to build out each subproblem. The user's\n",
    "        # function returns a dictionary mapping `Symbol`s to `State` objects,\n",
    "        # and an `Uncertainty` object.\n",
    "        states, uncertainty = subproblem_builder(model, t)\n",
    "        # Now add the cost-to-go terms:\n",
    "        JuMP.@variable(model, cost_to_go >= lower_bound)\n",
    "        obj = JuMP.objective_function(model)\n",
    "        JuMP.@objective(model, Min, obj + cost_to_go)\n",
    "        # If there are no outgoing arcs, the cost-to-go is 0.0.\n",
    "        if length(graph[t]) == 0\n",
    "            JuMP.fix(cost_to_go, 0.0; force = true)\n",
    "        end\n",
    "        push!(nodes, Node(model, states, uncertainty, cost_to_go))\n",
    "    end\n",
    "    return PolicyGraph(nodes, graph)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can create a model using the `subproblem_builder` function we defined\n",
    "earlier:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = PolicyGraph(\n",
    "    subproblem_builder;\n",
    "    graph = [Dict(2 => 1.0), Dict(3 => 1.0), Dict{Int,Float64}()],\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: helpful samplers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we get properly coding the solution algorithm, it's also going to be\n",
    "useful to have a function that samples a realization of the random variable\n",
    "defined by `Ω` and `P`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function sample_uncertainty(uncertainty::Uncertainty)\n",
    "    r = rand()\n",
    "    for (p, ω) in zip(uncertainty.P, uncertainty.Ω)\n",
    "        r -= p\n",
    "        if r < 0.0\n",
    "            return ω\n",
    "        end\n",
    "    end\n",
    "    return error(\"We should never get here because P should sum to 1.0.\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "    `rand()` samples a uniform random variable in `[0, 1)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for i in 1:3\n",
    "    println(\"ω = \", sample_uncertainty(model.nodes[1].uncertainty))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's also going to be useful to define a function that generates a random walk\n",
    "through the nodes of the graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function sample_next_node(model::PolicyGraph, current::Int)\n",
    "    if length(model.arcs[current]) == 0\n",
    "        # No outgoing arcs!\n",
    "        return nothing\n",
    "    else\n",
    "        r = rand()\n",
    "        for (to, probability) in model.arcs[current]\n",
    "            r -= probability\n",
    "            if r < 0.0\n",
    "                return to\n",
    "            end\n",
    "        end\n",
    "        # We looped through the outgoing arcs and still have probability left\n",
    "        # over! This means we've hit an implicit \"zero\" node.\n",
    "        return nothing\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for i in 1:3\n",
    "    # We use `repr` to print the next node, because `sample_next_node` can\n",
    "    # return `nothing`.\n",
    "    println(\"Next node from $(i) = \", repr(sample_next_node(model, i)))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a little boring, because our graph is simple. However, more\n",
    "complicated graphs will generate more interesting trajectories!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: the forward pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that, after approximating the cost-to-go term, we need a way of\n",
    "generating the cuts. As the first step, we need a way of generating candidate\n",
    "solutions $x_k^\\prime$. However, unlike the Kelley's example, our functions\n",
    "$V_j^k(x^\\prime, \\varphi)$ need two inputs: an outgoing state variable and a\n",
    "realization of the random variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One way of getting these inputs is just to pick a random (feasible) value.\n",
    "However, in doing so, we might pick outgoing state variables that we will\n",
    "never see in practice, or we might infrequently pick outgoing state variables\n",
    "that we will often see in practice. Therefore, a better way of generating the\n",
    "inputs is to use a simulation of the policy, which we call the **forward**\n",
    "**pass**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The forward pass walks the policy graph from start to end, transitioning\n",
    "randomly along the arcs. At each node, it observes a realization of the random\n",
    "variable and solves the approximated subproblem to generate a candidate\n",
    "outgoing state variable $x_k^\\prime$. The outgoing state variable is passed as\n",
    "the incoming state variable to the next node in the trajectory."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function forward_pass(model::PolicyGraph, io::IO = stdout)\n",
    "    println(io, \"| Forward Pass\")\n",
    "    # First, get the value of the state at the root node (e.g., x_R).\n",
    "    incoming_state =\n",
    "        Dict(k => JuMP.fix_value(v.in) for (k, v) in model.nodes[1].states)\n",
    "    # `simulation_cost` is an accumlator that is going to sum the stage-costs\n",
    "    # incurred over the forward pass.\n",
    "    simulation_cost = 0.0\n",
    "    # We also need to record the nodes visited and resultant outgoing state\n",
    "    # variables so we can pass them to the backward pass.\n",
    "    trajectory = Tuple{Int,Dict{Symbol,Float64}}[]\n",
    "    # Now's the meat of the forward pass: beginning at the first node:\n",
    "    t = 1\n",
    "    while t !== nothing\n",
    "        node = model.nodes[t]\n",
    "        println(io, \"| | Visiting node $(t)\")\n",
    "        # Sample the uncertainty:\n",
    "        ω = sample_uncertainty(node.uncertainty)\n",
    "        println(io, \"| | | ω = \", ω)\n",
    "        # Parameterizing the subproblem using the user-provided function:\n",
    "        node.uncertainty.parameterize(ω)\n",
    "        println(io, \"| | | x = \", incoming_state)\n",
    "        # Update the incoming state variable:\n",
    "        for (k, v) in incoming_state\n",
    "            JuMP.fix(node.states[k].in, v; force = true)\n",
    "        end\n",
    "        # Now solve the subproblem and check we found an optimal solution:\n",
    "        JuMP.optimize!(node.subproblem)\n",
    "        if JuMP.termination_status(node.subproblem) != JuMP.MOI.OPTIMAL\n",
    "            error(\"Something went terribly wrong!\")\n",
    "        end\n",
    "        # Compute the outgoing state variables:\n",
    "        outgoing_state = Dict(k => JuMP.value(v.out) for (k, v) in node.states)\n",
    "        println(io, \"| | | x′ = \", outgoing_state)\n",
    "        # We also need to compute the stage cost to add to our\n",
    "        # `simulation_cost` accumulator:\n",
    "        stage_cost =\n",
    "            JuMP.objective_value(node.subproblem) - JuMP.value(node.cost_to_go)\n",
    "        simulation_cost += stage_cost\n",
    "        println(io, \"| | | C(x, u, ω) = \", stage_cost)\n",
    "        # As a penultimate step, set the outgoing state of stage t and the\n",
    "        # incoming state of stage t + 1, and add the node to the trajectory.\n",
    "        incoming_state = outgoing_state\n",
    "        push!(trajectory, (t, outgoing_state))\n",
    "        # Finally, sample a new node to step to. If `t === nothing`, the\n",
    "        # `while` loop will break.\n",
    "        t = sample_next_node(model, t)\n",
    "    end\n",
    "    return trajectory, simulation_cost\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at one forward pass:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trajectory, simulation_cost = forward_pass(model);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: the backward pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the forward pass, we obtained a vector of nodes visited and their\n",
    "corresponding outgoing state variables. Now we need to refine the\n",
    "approximation for each node at the candidate solution for the outgoing state\n",
    "variable. That is, we need to add a new cut:\n",
    "$$\n",
    "\\theta \\ge \\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}\\left[V_j^k(x^\\prime_k, \\varphi) + \\frac{d}{dx^\\prime}V_j^k(x^\\prime_k, \\varphi)^\\top (x^\\prime - x^\\prime_k)\\right]\n",
    "$$\n",
    "or alternatively:\n",
    "$$\n",
    "\\theta \\ge \\sum\\limits_{j \\in i^+} \\sum\\limits_{\\varphi \\in \\Omega_j} p_{ij} p_{\\varphi}\\left[V_j^k(x^\\prime_k, \\varphi) + \\frac{d}{dx^\\prime}V_j^k(x^\\prime_k, \\varphi)^\\top (x^\\prime - x^\\prime_k)\\right]\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It doesn't matter what order we visit the nodes to generate these cuts for.\n",
    "For example, we could compute them all in parallel, using the current\n",
    "approximations of $V^K_i$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we can be smarter than that."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we traverse the list of nodes visited in the forward pass in reverse, then\n",
    "we come to refine the $i^{th}$ node in the trajectory, we will already have\n",
    "improved the approximation of the $(i+1)^{th}$ node in the trajectory as well!\n",
    "Therefore, our refinement of the $i^{th}$ node will be better than if we\n",
    "improved node $i$ first, and then refined node $(i+1)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we walk the nodes in reverse, we call this the **backward pass**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Info**\n",
    "    If you're into deep learning, you could view this as the equivalent of\n",
    "    back-propagation: the forward pass pushes primal information through the\n",
    "    graph (outgoing state variables), and the backward pass pulls dual\n",
    "    information (cuts) back through the graph to improve our decisions on the\n",
    "    next forward pass."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function backward_pass(\n",
    "    model::PolicyGraph,\n",
    "    trajectory::Vector{Tuple{Int,Dict{Symbol,Float64}}},\n",
    "    io::IO = stdout,\n",
    ")\n",
    "    println(io, \"| Backward pass\")\n",
    "    # For the backward pass, we walk back up the nodes.\n",
    "    for i in reverse(1:length(trajectory))\n",
    "        index, outgoing_states = trajectory[i]\n",
    "        node = model.nodes[index]\n",
    "        println(io, \"| | Visiting node $(index)\")\n",
    "        if length(model.arcs[index]) == 0\n",
    "            # If there are no children, the cost-to-go is 0.\n",
    "            println(io, \"| | | Skipping node because the cost-to-go is 0\")\n",
    "            continue\n",
    "        end\n",
    "        # Create an empty affine expression that we will use to build up the\n",
    "        # right-hand side of the cut expression.\n",
    "        cut_expression = JuMP.AffExpr(0.0)\n",
    "        # For each node j ∈ i⁺\n",
    "        for (j, P_ij) in model.arcs[index]\n",
    "            next_node = model.nodes[j]\n",
    "            # Set the incoming state variables of node j to the outgoing state\n",
    "            # variables of node i\n",
    "            for (k, v) in outgoing_states\n",
    "                JuMP.fix(next_node.states[k].in, v; force = true)\n",
    "            end\n",
    "            # Then for each realization of φ ∈ Ωⱼ\n",
    "            for (pφ, φ) in zip(next_node.uncertainty.P, next_node.uncertainty.Ω)\n",
    "                # Setup and solve for the realization of φ\n",
    "                println(io, \"| | | Solving φ = \", φ)\n",
    "                next_node.uncertainty.parameterize(φ)\n",
    "                JuMP.optimize!(next_node.subproblem)\n",
    "                # Then prepare the cut `P_ij * pφ * [V + dVdxᵀ(x - x_k)]``\n",
    "                V = JuMP.objective_value(next_node.subproblem)\n",
    "                println(io, \"| | | | V = \", V)\n",
    "                dVdx = Dict(\n",
    "                    k => JuMP.reduced_cost(v.in) for (k, v) in next_node.states\n",
    "                )\n",
    "                println(io, \"| | | | dVdx′ = \", dVdx)\n",
    "                cut_expression += JuMP.@expression(\n",
    "                    node.subproblem,\n",
    "                    P_ij *\n",
    "                    pφ *\n",
    "                    (\n",
    "                        V + sum(\n",
    "                            dVdx[k] * (x.out - outgoing_states[k]) for\n",
    "                            (k, x) in node.states\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            end\n",
    "        end\n",
    "        # And then refine the cost-to-go variable by adding the cut:\n",
    "        c = JuMP.@constraint(node.subproblem, node.cost_to_go >= cut_expression)\n",
    "        println(io, \"| | | Adding cut : \", c)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: bounds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lower bounds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall from Kelley's that we can obtain a lower bound for $f(x^*)$ be\n",
    "evaluating $f^K$. The analogous lower bound for a multistage stochastic\n",
    "program is:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\mathbb{E}_{i \\in R^+, \\omega \\in \\Omega_i}[V_i^K(x_R, \\omega)] \\le \\min_{\\pi} \\mathbb{E}_{i \\in R^+, \\omega \\in \\Omega_i}[V_i^\\pi(x_R, \\omega)]\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's how we compute the lower bound:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function lower_bound(model::PolicyGraph)\n",
    "    node = model.nodes[1]\n",
    "    bound = 0.0\n",
    "    for (p, ω) in zip(node.uncertainty.P, node.uncertainty.Ω)\n",
    "        node.uncertainty.parameterize(ω)\n",
    "        JuMP.optimize!(node.subproblem)\n",
    "        bound += p * JuMP.objective_value(node.subproblem)\n",
    "    end\n",
    "    return bound\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "    The implementation is simplified because we assumed that there is only one\n",
    "    arc from the root node, and that it pointed to the first node in the\n",
    "    vector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we haven't trained a policy yet, the lower bound is going to be very\n",
    "bad:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lower_bound(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upper bounds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Kelley's algorithm, we could easily construct an upper bound by\n",
    "evaluating $f(x_K)$. However, it is almost always intractable to evaluate an\n",
    "upper bound for multistage stochastic programs due to the large number of\n",
    "nodes and the nested expectations. Instead, we can perform a Monte Carlo\n",
    "simulation of the policy to build a statistical estimate for the value of\n",
    "$\\mathbb{E}_{i \\in R^+, \\omega \\in \\Omega_i}[V_i^\\pi(x_R, \\omega)]$, where\n",
    "$\\pi$ is the policy defined by the current approximations $V^K_i$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function upper_bound(model::PolicyGraph; replications::Int)\n",
    "    # Pipe the output to `devnull` so we don't print too much!\n",
    "    simulations = [forward_pass(model, devnull) for i in 1:replications]\n",
    "    z = [s[2] for s in simulations]\n",
    "    μ = Statistics.mean(z)\n",
    "    tσ = 1.96 * Statistics.std(z) / sqrt(replications)\n",
    "    return μ, tσ\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "    The width of the confidence interval is incorrect if there are cycles in\n",
    "    the graph, because the distribution of simulation costs `z` is not\n",
    "    symmetric. The mean is correct, however."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Termination criteria"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Kelley's algorithm, the upper bound was deterministic. Therefore, we could\n",
    "terminate the algorithm when the lower bound was sufficiently close to the\n",
    "upper bound. However, our upper bound for SDDP is not deterministic; it is a\n",
    "confidence interval!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some people suggest terminating SDDP when the lower bound is contained within\n",
    "the confidence interval. However, this is a poor choice because it is too easy\n",
    "to generate a false positive. For example, if we use a small number of\n",
    "replications then the width of the confidence will be large, and we are more\n",
    "likely to terminate!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a future tutorial (not yet written...) we will discuss termination criteria\n",
    "in more depth. For now, pick a large number of iterations and train for as\n",
    "long as possible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tip**\n",
    "    For a rule of thumb, pick a large number of iterations to train the\n",
    "    policy for (e.g.,\n",
    "    $10 \\times |\\mathcal{N}| \\times \\max\\limits_{i\\in\\mathcal{N}} |\\Omega_i|$)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: the training loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `train` loop of SDDP just applies the forward and backward passes\n",
    "iteratively, followed by a final simulation to compute the upper bound\n",
    "confidence interval:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train(\n",
    "    model::PolicyGraph;\n",
    "    iteration_limit::Int,\n",
    "    replications::Int,\n",
    "    io::IO = stdout,\n",
    ")\n",
    "    for i in 1:iteration_limit\n",
    "        println(io, \"Starting iteration $(i)\")\n",
    "        outgoing_states, _ = forward_pass(model, io)\n",
    "        backward_pass(model, outgoing_states, io)\n",
    "        println(io, \"| Finished iteration\")\n",
    "        println(io, \"| | lower_bound = \", lower_bound(model))\n",
    "    end\n",
    "    println(io, \"Termination status: iteration limit\")\n",
    "    μ, tσ = upper_bound(model; replications = replications)\n",
    "    println(io, \"Upper bound = $(μ) ± $(tσ)\")\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using our `model` we defined earlier, we can go:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train(model; iteration_limit = 3, replications = 100)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Success! We trained a policy for a finite horizon multistage stochastic\n",
    "program using stochastic dual dynamic programming."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation: evaluating the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A final step is the ability to evaluate the policy at a given point."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function evaluate_policy(\n",
    "    model::PolicyGraph;\n",
    "    node::Int,\n",
    "    incoming_state::Dict{Symbol,Float64},\n",
    "    random_variable,\n",
    ")\n",
    "    the_node = model.nodes[node]\n",
    "    the_node.uncertainty.parameterize(random_variable)\n",
    "    for (k, v) in incoming_state\n",
    "        JuMP.fix(the_node.states[k].in, v; force = true)\n",
    "    end\n",
    "    JuMP.optimize!(the_node.subproblem)\n",
    "    return Dict(\n",
    "        k => JuMP.value.(v) for\n",
    "        (k, v) in JuMP.object_dictionary(the_node.subproblem)\n",
    "    )\n",
    "end\n",
    "\n",
    "evaluate_policy(\n",
    "    model;\n",
    "    node = 1,\n",
    "    incoming_state = Dict(:volume => 150.0),\n",
    "    random_variable = 75,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "    The random variable can be **out-of-sample**, i.e., it doesn't have to be\n",
    "    in the vector $\\Omega$ we created when defining the model! This is a\n",
    "    notable difference to other multistage stochastic solution methods like\n",
    "    progressive hedging or using the deterministic equivalent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: infinite horizon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As promised earlier, our implementation is actually pretty general. It can\n",
    "solve any multistage stochastic (linear) program defined by a policy graph,\n",
    "including infinite horizon problems!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's an example, where we have extended our earlier problem with an arc from\n",
    "node 3 to node 2 with probability 0.5. You can interpret the 0.5 as a discount\n",
    "factor."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = PolicyGraph(\n",
    "    subproblem_builder;\n",
    "    graph = [Dict(2 => 1.0), Dict(3 => 1.0), Dict(2 => 0.5)],\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, train a policy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train(model; iteration_limit = 3, replications = 100)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Success! We trained a policy for an infinite horizon multistage stochastic\n",
    "program using stochastic dual dynamic programming. Note how some of the\n",
    "forward passes are different lengths!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate_policy(\n",
    "    model;\n",
    "    node = 3,\n",
    "    incoming_state = Dict(:volume => 100.0),\n",
    "    random_variable = 10.0,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The knapsack problem example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to formulate and solve a\n",
    "simple optimization problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial requires the following packages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "import HiGHS"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem)\n",
    "is a classical optimization problem: given a set of items and a container with\n",
    "a fixed capacity, choose a subset of items having the greatest combined\n",
    "value that will fit within the container without exceeding the capacity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The name of the problem suggests its analogy to packing for a trip,\n",
    "where the baggage weight limit is the capacity and the goal is to pack the\n",
    "most profitable combination of belongings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can formulate the knapsack problem as the integer linear program:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max \\; & \\sum_{i=1}^n c_i x_i      \\\\\n",
    "s.t. \\; & \\sum_{i=1}^n w_i x_i \\le C, \\\\\n",
    "        & x_i \\in \\{0,1\\},\\quad \\forall i=1,\\ldots,n,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $C$ is the capacity, and there is a choice between $n$ items, with\n",
    "item $i$ having weight $w_i$, profit $c_i$. Decision variable $x_i$ is\n",
    "equal to 1 if the item is chosen and 0 if not."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This formulation can be written more compactly as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max \\; & c^\\top x       \\\\\n",
    "s.t. \\; & w^\\top x \\le C \\\\\n",
    "        & x \\text{ binary }.\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data for the problem consists of two vectors (one for the profits and one\n",
    "for the weights) along with a capacity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are five objects:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 5;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our example, we use a capacity of 10 units:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "capacity = 10.0;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and the profit and cost data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "profit = [5.0, 3.0, 2.0, 7.0, 4.0];\n",
    "weight = [2.0, 8.0, 4.0, 2.0, 5.0];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JuMP formulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's begin constructing the JuMP model for our knapsack problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we'll create a `Model` object for holding model elements as we\n",
    "construct each part. We'll also set the solver that will ultimately be called\n",
    "to solve the model, once it's constructed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Model(HiGHS.Optimizer)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we need the decision variables representing which items are chosen:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variable(model, x[1:n], Bin)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now want to constrain those variables so that their combined\n",
    "weight is less than or equal to the given capacity:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@constraint(model, sum(weight[i] * x[i] for i in 1:n) <= capacity)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, our objective is to maximize the combined profit of the chosen items:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@objective(model, Max, sum(profit[i] * x[i] for i in 1:n))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's print a human-readable description of the model and check that the model\n",
    "looks as expected:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now solve the optimization problem and inspect the results."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "optimize!(model)\n",
    "solution_summary(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The items chosen are"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "items_chosen = [i for i in 1:n if value(x[i]) > 0.5]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing a function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After working interactively, it is good practice to implement your model in a\n",
    "function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function can be used to ensure that the model is given well-defined input\n",
    "data with validation checks, and that the solution process went as expected."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_knapsack_problem(;\n",
    "    profit::Vector{Float64},\n",
    "    weight::Vector{Float64},\n",
    "    capacity::Float64,\n",
    ")\n",
    "    n = length(weight)\n",
    "    # The profit and weight vectors must be of equal length.\n",
    "    @assert length(profit) == n\n",
    "    model = Model(HiGHS.Optimizer)\n",
    "    set_silent(model)\n",
    "    @variable(model, x[1:n], Bin)\n",
    "    @objective(model, Max, profit' * x)\n",
    "    @constraint(model, weight' * x <= capacity)\n",
    "    optimize!(model)\n",
    "    @assert termination_status(model) == OPTIMAL\n",
    "    @assert primal_status(model) == FEASIBLE_POINT\n",
    "    println(\"Objective is: \", objective_value(model))\n",
    "    println(\"Solution is:\")\n",
    "    for i in 1:n\n",
    "        print(\"x[$i] = \", round(Int, value(x[i])))\n",
    "        println(\", c[$i] / w[$i] = \", profit[i] / weight[i])\n",
    "    end\n",
    "    chosen_items = [i for i in 1:n if value(x[i]) > 0.5]\n",
    "    return return chosen_items\n",
    "end\n",
    "\n",
    "solve_knapsack_problem(; profit = profit, weight = weight, capacity = capacity)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe that the chosen items (1, 4, and 5) have the best\n",
    "profit to weight ratio in this particular example."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are some things to try next:\n",
    "\n",
    "* Call the function with different data. What happens as the capacity\n",
    "  increases?\n",
    "* What happens if the profit and weight vectors are different lengths?\n",
    "* Instead of creating a binary variable with `Bin`, we could have written\n",
    "  `@variable(model, 0 <= x[1:n] <= 1, Int)`. Verify that this formulation\n",
    "  finds the same solution. What happens if we are allowed to take more than\n",
    "  one of each item?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The diet problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to incorporate DataFrames\n",
    "into a JuMP model. As an example, we use classic [Stigler diet problem](https://en.wikipedia.org/wiki/Stigler_diet)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial requires the following packages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "import CSV\n",
    "import DataFrames\n",
    "import HiGHS\n",
    "import Test"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We wish to cook a nutritionally balanced meal by choosing the quantity of each\n",
    "food $f$ to eat from a set of foods $F$ in our kitchen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each food $f$ has a cost, $c_f$, as well as a macro-nutrient profile\n",
    "$a_{m,f}$ for each macro-nutrient $m \\in M$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we care about a nutritionally balanced meal, we set some minimum and\n",
    "maximum limits for each nutrient, which we denote $l_m$ and $u_m$\n",
    "respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Furthermore, because we are optimizers, we seek the minimum cost solution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With a little effort, we can formulate our dinner problem as the following\n",
    "linear program:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min & \\sum\\limits_{f \\in F} c_f x_f \\\\\n",
    "\\text{s.t.}\\ \\ & l_m \\le \\sum\\limits_{f \\in F} a_{m,f} x_f \\le u_m, && \\forall m \\in M \\\\\n",
    "& x_f \\ge 0, && \\forall f \\in F.\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the rest of this tutorial, we will create and solve this problem in JuMP,\n",
    "and learn what we should cook for dinner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need some data for the problem. For this tutorial, we'll write CSV\n",
    "files to a temporary directory from Julia. If you have existing files, you\n",
    "could change the filenames to point to them instead."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dir = mktempdir()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first file is a list of foods with their macro-nutrient profile:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "food_csv_filename = joinpath(dir, \"diet_foods.csv\")\n",
    "open(food_csv_filename, \"w\") do io\n",
    "    write(\n",
    "        io,\n",
    "        \"\"\"\n",
    "        name,cost,calories,protein,fat,sodium\n",
    "        hamburger,2.49,410,24,26,730\n",
    "        chicken,2.89,420,32,10,1190\n",
    "        hot dog,1.50,560,20,32,1800\n",
    "        fries,1.89,380,4,19,270\n",
    "        macaroni,2.09,320,12,10,930\n",
    "        pizza,1.99,320,15,12,820\n",
    "        salad,2.49,320,31,12,1230\n",
    "        milk,0.89,100,8,2.5,125\n",
    "        ice cream,1.59,330,8,10,180\n",
    "        \"\"\",\n",
    "    )\n",
    "    return\n",
    "end\n",
    "foods = CSV.read(food_csv_filename, DataFrames.DataFrame)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, $F$ is `foods.name` and $c_f$ is `foods.cost`. (We're also playing\n",
    "a bit loose the term \"macro-nutrient\" by including calories and sodium.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need our minimum and maximum limits:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nutrient_csv_filename = joinpath(dir, \"diet_nutrient.csv\")\n",
    "open(nutrient_csv_filename, \"w\") do io\n",
    "    write(\n",
    "        io,\n",
    "        \"\"\"\n",
    "        nutrient,min,max\n",
    "        calories,1800,2200\n",
    "        protein,91,\n",
    "        fat,0,65\n",
    "        sodium,0,1779\n",
    "        \"\"\",\n",
    "    )\n",
    "    return\n",
    "end\n",
    "limits = CSV.read(nutrient_csv_filename, DataFrames.DataFrame)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Protein is missing data for the maximum. Let's fix that using `coalesce`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "limits.max = coalesce.(limits.max, Inf)\n",
    "limits"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JuMP formulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we're ready to convert our mathematical formulation into a JuMP model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create a new JuMP model. Since we have a linear program, we'll use\n",
    "HiGHS as our optimizer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Model(HiGHS.Optimizer)\n",
    "set_silent(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create a set of decision variables `x`, with one element for each row\n",
    "in the DataFrame, and each `x` has a lower bound of `0`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variable(model, x[foods.name] >= 0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To simplify things later on, we store the vector as a new column `x` in the\n",
    "DataFrame `foods`. Since `x` is a `DenseAxisArray`, we first need to convert\n",
    "it to an `Array`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "foods.x = Array(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our objective is to minimize the total cost of purchasing food:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@objective(model, Min, sum(foods.cost .* foods.x));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the next component, we need to add a constraint that our total intake of\n",
    "each component is within the limits contained in the `limits` DataFrame:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@constraint(\n",
    "    model,\n",
    "    [row in eachrow(limits)],\n",
    "    row.min <= sum(foods[!, row.nutrient] .* foods.x) <= row.max,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does our model look like?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's optimize and take a look at the solution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "optimize!(model)\n",
    "Test.@test primal_status(model) == FEASIBLE_POINT\n",
    "Test.@test objective_value(model) ≈ 11.8288 atol = 1e-4\n",
    "solution_summary(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We found an optimal solution. Let's see what the optimal solution is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for row in eachrow(foods)\n",
    "    println(row.name, \" = \", value(row.x))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's a lot of milk and ice cream, and sadly, we only get `0.6` of a\n",
    "hamburger."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also use the function `Containers.rowtable` to easily convert\n",
    "the result into a DataFrame:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "table = Containers.rowtable(value, x; header = [:food, :quantity])\n",
    "solution = DataFrames.DataFrame(table)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This makes it easy to perform analyses our solution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "filter!(row -> row.quantity > 0.0, solution)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem modification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "JuMP makes it easy to take an existing model and modify it by adding extra\n",
    "constraints. Let's see what happens if we add a constraint that we can buy at\n",
    "most 6 units of milk or ice cream combined."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dairy_foods = [\"milk\", \"ice cream\"]\n",
    "is_dairy = map(name -> name in dairy_foods, foods.name)\n",
    "dairy_constraint = @constraint(model, sum(foods[is_dairy, :x]) <= 6)\n",
    "optimize!(model)\n",
    "Test.@test termination_status(model) == INFEASIBLE\n",
    "Test.@test primal_status(model) == NO_SOLUTION\n",
    "solution_summary(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There exists no feasible solution to our problem. Looks like we're stuck\n",
    "eating ice cream for dinner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* You can delete a constraint using `delete(model, dairy_constraint)`. Can you\n",
    "  add a different constraint to provide a diet with less dairy?\n",
    "* Some food items (like hamburgers) are discrete. You can use `set_integer`\n",
    "  to force a variable to take integer values. What happens to the solution if\n",
    "  you do?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Two-stage stochastic programs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to model and solve a\n",
    "two-stage stochastic program."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial uses the following packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "import Distributions\n",
    "import HiGHS\n",
    "import Plots\n",
    "import StatsPlots\n",
    "import Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "During the week, you are a busy practitioner of Operations Research. To escape\n",
    "the drudgery of mathematics, you decide to open a side business selling creamy\n",
    "mushroom pies with puff pastry. After a few weeks, it quickly becomes apparent\n",
    "that operating a food business is not so easy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pies must be prepared in the morning, _before_ you open for the day and\n",
    "can gauge the level of demand. If you bake too many, the unsold pies at the\n",
    "end of the day must be discarded and you have wasted time and money on their\n",
    "production. But if you bake too few, then there may be un-served customers and\n",
    "you could have made more money by baking more pies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After a few weeks of poor decision making, you decide to put your knowledge of\n",
    "Operations Research to good use, starting with some data collection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each pie costs you \\$2 to make, and you sell them at \\$5 each. Disposal of an\n",
    "unsold pie costs \\$0.10. Based on three weeks of data collected, in which you\n",
    "made 200 pies each week, you sold 150, 190, and 200 pies. Thus, as a guess,\n",
    "you assume a triangular distribution of demand with a minimum of 150, a median\n",
    "of 200, and a maximum of 250."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can model this problem by a two-stage stochastic program. In the first\n",
    "stage, we decide a quantity of pies to make $x$. We make this decision\n",
    "before we observe the demand $d_\\omega$. In the second stage, we sell\n",
    "$y_\\omega$ pies, and incur any costs for unsold pies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can formulate this problem as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max\\limits_{x,y_\\omega} \\;\\; & -2x + \\mathbb{E}_\\omega[5y_\\omega - 0.1(x - y_\\omega)] \\\\\n",
    "  & y_\\omega \\le x              & \\quad \\forall \\omega \\in \\Omega \\\\\n",
    "  & 0 \\le y_\\omega \\le d_\\omega & \\quad \\forall \\omega \\in \\Omega \\\\\n",
    "  & x \\ge 0.\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Average approximation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the distribution of demand is continuous, then our problem has an infinite\n",
    "number of variables and constraints. To form a computationally tractable\n",
    "problem, we instead use a finite set of samples drawn from the distribution.\n",
    "This is called sample average approximation (SAA)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "D = Distributions.TriangularDist(150.0, 250.0, 200.0)\n",
    "N = 100\n",
    "d = sort!(rand(D, N));\n",
    "Ω = 1:N\n",
    "P = fill(1 / N, N);\n",
    "StatsPlots.histogram(d; bins = 20, label = \"\", xlabel = \"Demand\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JuMP model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The implementation of our two-stage stochastic program in JuMP is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Model(HiGHS.Optimizer)\n",
    "set_silent(model)\n",
    "@variable(model, x >= 0)\n",
    "@variable(model, 0 <= y[ω in Ω] <= d[ω])\n",
    "@constraint(model, [ω in Ω], y[ω] <= x)\n",
    "@expression(model, z[ω in Ω], 5y[ω] - 0.1 * (x - y[ω]))\n",
    "@objective(model, Max, -2x + sum(P[ω] * z[ω] for ω in Ω))\n",
    "optimize!(model)\n",
    "solution_summary(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimal number of pies to make is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "value(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of total profit is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "total_profit = [-2 * value(x) + value(z[ω]) for ω in Ω]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    bin_distribution(x::Vector{Float64}, N::Int)\n",
    "\n",
    "A helper function that discretizes `x` into bins of width `N`.\n",
    "\"\"\"\n",
    "bin_distribution(x, N) = N * (floor(minimum(x) / N):ceil(maximum(x) / N))\n",
    "\n",
    "plot = StatsPlots.histogram(\n",
    "    total_profit;\n",
    "    bins = bin_distribution(total_profit, 25),\n",
    "    label = \"\",\n",
    "    xlabel = \"Profit [\\$]\",\n",
    "    ylabel = \"Number of outcomes\",\n",
    ")\n",
    "μ = Statistics.mean(total_profit)\n",
    "Plots.vline!(\n",
    "    plot,\n",
    "    [μ];\n",
    "    label = \"Expected profit (\\$$(round(Int, μ)))\",\n",
    "    linewidth = 3,\n",
    ")\n",
    "plot"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risk measures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A risk measure is a function which maps a random variable to a real number.\n",
    "Common risk measures include the mean (expectation), median, mode, and\n",
    "maximum. We need a risk measure to convert the distribution of second stage\n",
    "costs into a single number that can be optimized."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model currently uses the expectation risk measure, but others are possible\n",
    "too. One popular risk measure is the conditional value at risk (CVaR)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CVaR has a parameter $\\gamma$, and it computes the expectation of the worst\n",
    "$\\gamma$ fraction of outcomes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we are maximizing, so that small outcomes are bad, the definition of CVaR\n",
    "is:\n",
    "$$\n",
    "CVaR_{\\gamma}[Z] = \\max\\limits_{\\xi} \\;\\; \\xi - \\frac{1}{\\gamma}\\mathbb{E}_\\omega\\left[(\\xi - Z)_+\\right]\n",
    "$$\n",
    "which can be formulated as the linear program:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "CVaR_{\\gamma}[Z] = \\max\\limits_{\\xi, z_\\omega} \\;\\; & \\xi - \\frac{1}{\\gamma}\\sum P_\\omega z_\\omega\\\\\n",
    " & z_\\omega \\ge \\xi - Z_\\omega & \\quad \\forall \\omega \\\\\n",
    " & z_\\omega \\ge 0 & \\quad \\forall \\omega.\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function CVaR(Z::Vector{Float64}, P::Vector{Float64}; γ::Float64)\n",
    "    @assert 0 < γ <= 1\n",
    "    N = length(Z)\n",
    "    model = Model(HiGHS.Optimizer)\n",
    "    set_silent(model)\n",
    "    @variable(model, ξ)\n",
    "    @variable(model, z[1:N] >= 0)\n",
    "    @constraint(model, [i in 1:N], z[i] >= ξ - Z[i])\n",
    "    @objective(model, Max, ξ - 1 / γ * sum(P[i] * z[i] for i in 1:N))\n",
    "    optimize!(model)\n",
    "    return objective_value(model)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "When `γ` is `1.0`, we compute the mean of the profit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cvar_10 = CVaR(total_profit, P; γ = 1.0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Statistics.mean(total_profit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As `γ` approaches `0.0`, we compute the worst-case (minimum) profit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cvar_00 = CVaR(total_profit, P; γ = 0.0001)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "minimum(total_profit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "By varying `γ` between `0` and `1` we can compute some trade-off of these two\n",
    "extremes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cvar_05 = CVaR(total_profit, P; γ = 0.5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot these outcomes on our distribution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot = StatsPlots.histogram(\n",
    "    total_profit;\n",
    "    bins = bin_distribution(total_profit, 25),\n",
    "    label = \"\",\n",
    "    xlabel = \"Profit [\\$]\",\n",
    "    ylabel = \"Number of outcomes\",\n",
    ")\n",
    "Plots.vline!(\n",
    "    plot,\n",
    "    [cvar_10 cvar_05 cvar_00];\n",
    "    label = [\"γ = 1.0\" \"γ = 0.5\" \"γ = 0.0\"],\n",
    "    linewidth = 3,\n",
    ")\n",
    "plot"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risk averse sample average approximation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because CVaR can be formulated as a linear program, we can form a risk averse\n",
    "sample average approximation model by combining the two formulations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "γ = 0.4\n",
    "model = Model(HiGHS.Optimizer)\n",
    "set_silent(model)\n",
    "@variable(model, x >= 0)\n",
    "@variable(model, 0 <= y[ω in Ω] <= d[ω])\n",
    "@constraint(model, [ω in Ω], y[ω] <= x)\n",
    "@expression(model, Z[ω in Ω], 5 * y[ω] - 0.1(x - y[ω]))\n",
    "@variable(model, ξ)\n",
    "@variable(model, z[ω in Ω] >= 0)\n",
    "@constraint(model, [ω in Ω], z[ω] >= ξ - Z[ω])\n",
    "@objective(model, Max, -2x + ξ - 1 / γ * sum(P[ω] * z[ω] for ω in Ω))\n",
    "optimize!(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "When $\\gamma = 0.4$, the optimal number of pies to bake is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "value(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of total profit is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "risk_averse_total_profit = [value(-2x + Z[ω]) for ω in Ω]\n",
    "bins = bin_distribution([total_profit; risk_averse_total_profit], 25)\n",
    "plot = StatsPlots.histogram(total_profit; label = \"Expectation\", bins = bins)\n",
    "StatsPlots.histogram!(\n",
    "    plot,\n",
    "    risk_averse_total_profit;\n",
    "    label = \"CV@R\",\n",
    "    bins = bins,\n",
    "    alpha = 0.5,\n",
    ")\n",
    "plot"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    " * Try solving this problem for different numbers of samples and different\n",
    "   distributions.\n",
    " * Refactor the example to avoid hard-coding the costs. What happens to the\n",
    "   solution if the cost of disposing unsold pies increases?\n",
    " * Plot the optimal number of pies to make for different values of the risk\n",
    "   aversion parameter $\\gamma$. What is the relationship?"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
